{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nettoyage variables\n",
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#read csv file\n",
    "data = pd.read_csv('data/stat_acc_V3_cleared.csv', sep=';')\n",
    "new_data = pd.read_csv('data/stat_acc_V3_new.csv', sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#executer le code du fichier preparation.py\n",
    "%run preparation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribut 1: an_nais\n",
      "Attribut 2: age\n",
      "\n",
      "Nombre de colonnes avant suppression:  21\n",
      "Nombre de colonnes après suppression:  15\n",
      "Pourcentage de réduction:  28.57 %\n"
     ]
    }
   ],
   "source": [
    "# Identifier les attributs fortement corrélés\n",
    "\n",
    "# Créer une matrice de corrélation\n",
    "corr_matrix = new_data.corr().abs()\n",
    "\n",
    "# Identifiez les paires d'attributs fortement corrélés\n",
    "threshold = 0.8  # Définissez votre propre seuil de corrélation\n",
    "highly_correlated_pairs = []\n",
    "\n",
    "# Parcourir la matrice de corrélation\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[i, j]) >= threshold:\n",
    "            pair = (corr_matrix.columns[i], corr_matrix.columns[j])\n",
    "            highly_correlated_pairs.append(pair)\n",
    "\n",
    "#verifier les paires d'attributs fortement corrélés\n",
    "# print(highly_correlated_pairs)\n",
    "\n",
    "# Afficher les paires d'attributs fortement corrélés\n",
    "for pair in highly_correlated_pairs:\n",
    "    print(\"Attribut 1:\", pair[0])\n",
    "    print(\"Attribut 2:\", pair[1])\n",
    "\n",
    "    #suppression d'une des deux colonnes fortement corrélées (ici on supprime la colonne 'an_nais' jugée moins pertinente)\n",
    "    new_data = new_data.drop(pair[0], axis=1)\n",
    "\n",
    "    print()\n",
    "\n",
    "#export csv new_data\n",
    "new_data.to_csv('data/stat_acc_V3_new.csv', sep=';', index=False)\n",
    "\n",
    "#obtenir le nombre de colonnes avant et après suppression\n",
    "print(\"Nombre de colonnes avant suppression: \", len(data.columns))\n",
    "print(\"Nombre de colonnes après suppression: \", len(new_data.columns))\n",
    "\n",
    "#print pourcentage de réduction\n",
    "print(\"Pourcentage de réduction: \", round((len(data.columns) - len(new_data.columns)) / len(data.columns) * 100, 2), \"%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus : PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Réduction de dimension avec PCA sur le dataset data avec graphique de visualisation avant et après réduction\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering avec sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "# https://www.kaggle.com/code/lucaspcarlini/clustering-and-visualisation-using-folium-maps\n",
    "import plotly.graph_objects as go\n",
    "import webbrowser\n",
    "from sklearn.cluster import KMeans\n",
    "coordinates = data[['latitude', 'longitude']]\n",
    "# for make cluster variable\n",
    "n = 5 # 5 semble adéquat pour avoir DOM TOM + séparation nord sud centre France\n",
    "kmeans = KMeans(n_clusters=n, random_state=0, n_init=\"auto\").fit(coordinates)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "new_data['cluster_labels'] = labels # get label from dataframe\n",
    "print(new_data['cluster_labels'].value_counts())\n",
    "\n",
    "#preds = kmeans.fit_predict(new_data)\n",
    "\n",
    "\n",
    "# Centroide \n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# graphiques de visualisation des clusters, avec les attributs latitude et longitude + centroide\n",
    "#plt.scatter(new_data['latitude'], new_data['longitude'], c=new_data['cluster_labels'], s=50, cmap='viridis')\n",
    "#plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=100)\n",
    "#plt.show()\n",
    "#predicted_labels, cluster_centroids = kmeans_clustering(coordinates, n_clusters)\n",
    "\n",
    "# Création de la figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Ajout des points de données\n",
    "for cluster_label in range(n):\n",
    "    cluster_data = new_data[new_data['cluster_labels'] == cluster_label]\n",
    "    fig.add_trace(go.Scattergeo(\n",
    "        lon = cluster_data['longitude'],\n",
    "        lat = cluster_data['latitude'],\n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 8,\n",
    "            color = cluster_label,\n",
    "            line = dict(width = 1, color = 'rgba(0, 0, 0, 0.5)'),\n",
    "            opacity = 0.8\n",
    "        ),\n",
    "        name = f'Cluster {cluster_label}'\n",
    "    ))\n",
    "\n",
    "# Mise en forme de la carte\n",
    "fig.update_layout(\n",
    "    title = 'Carte des clusters via K-Means sklearn',\n",
    "    geo = dict(\n",
    "        resolution = 110,\n",
    "        showland = True,\n",
    "        showlakes = True,\n",
    "        landcolor = 'rgb(243, 243, 243)',\n",
    "        countrycolor = 'rgb(204, 204, 204)',\n",
    "        lakecolor = 'rgb(255, 255, 255)',\n",
    "        projection_type = 'natural earth'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.write_html('map_clusters.html')\n",
    "webbrowser.open(\"map_clusters.html\")\n",
    "print('map_clusters.html')\n",
    "\n",
    "preds = kmeans.fit_predict(new_data)\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_avg = silhouette_score(new_data, preds)\n",
    "print(\"Silhouette Coefficient :\", silhouette_avg)\n",
    "\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "ch_score = calinski_harabasz_score(new_data, preds)\n",
    "print(\"Calinski-Harabasz Index :\", ch_score)\n",
    "\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "db_score = davies_bouldin_score(new_data, preds)\n",
    "print(\"Davies-Bouldin Index :\", db_score)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering en scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Silhouette Coefficient entre -1 et 1 plus on est proche de 1 plus on est précis \n",
    "- davies_bouldin entre 0 et + inf plus on est proche de 0 mieux c'est \n",
    "- calinski_harabas entre 0 et + inf plus on est proche de inf mieux c'est "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import webbrowser\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dist Manhattan\n",
    "def distL1(lat1, lon1, lat2, lon2):\n",
    "    return abs(lat2 - lat1) + abs(lon2 - lon1)\n",
    "\n",
    "# Dist euclidienne\n",
    "def distL2(lat1, lon1, lat2, lon2):\n",
    "    return math.sqrt((lat2 - lat1)**2 + (lon2 - lon1)**2)\n",
    "\n",
    "# Dist Haversine (distance entre 2 pts sur la Terre)\n",
    "def distHaversine(lat1, lon1, lat2, lon2):\n",
    "    radius = 6371  # Rayon de la Terre en kilomètres\n",
    "    dlat = math.radians(lat2 - lat1)    # distance btw 2 pts en radians\n",
    "    dlon = math.radians(lon2 - lon1)\n",
    "    a = math.sin(dlat / 2) * math.sin(dlat / 2) + math.cos(math.radians(lat1)) \\\n",
    "        * math.cos(math.radians(lat2)) * math.sin(dlon / 2) * math.sin(dlon / 2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    d = radius * c\n",
    "    return d\n",
    "\n",
    "def kmeans_clustering(new_data, n_clusters, max_iterations=100):\n",
    "    np.random.seed(0)\n",
    "    centroids = new_data[np.random.choice(range(len(new_data)), size=n_clusters, replace=False)]\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        labels = np.zeros(len(new_data)) # Init labels pour chaque point\n",
    "         # Assignation des points aux clusters les plus proches\n",
    "        for i, point in enumerate(new_data):\n",
    "            min_distance = float('inf')\n",
    "            for j, centroid in enumerate(centroids):\n",
    "                dist = distL1(point[0], point[1], centroid[0], centroid[1]) # Manhattan distance\n",
    "                #dist = distL2(point[0], point[1], centroid[0], centroid[1])# euclienean distance\n",
    "                #dist = distHaversine(point[0], point[1], centroid[0], centroid[1]) # dist btw point and centroid\n",
    "                if dist < min_distance:\n",
    "                    min_distance = dist\n",
    "                    labels[i] = j  # Attribution du label du cluster au point\n",
    "        # Update centroids \n",
    "        new_centroids = np.zeros_like(centroids) # Calcul des nouveaux centroids en prenant la moyenne des points dans chaque cluster\n",
    "        cluster_counts = np.zeros(n_clusters)\n",
    "        for i in range(len(new_data)):\n",
    "            cluster_index = int(labels[i])\n",
    "            new_centroids[cluster_index] += new_data[i] # add coord point to cluster\n",
    "            cluster_counts[cluster_index] += 1 # Count  nb point in each cluster\n",
    "        for j in range(n_clusters):\n",
    "            if cluster_counts[j] > 0: # if cluster not empty to avoid div by 0 \n",
    "                    new_centroids[j] = np.divide(new_centroids[j], cluster_counts[j]) # average of point coord to get new centroid by dividing by nb point in cluster\n",
    "        \n",
    "        if np.allclose(centroids, new_centroids): # compare if old and new centroids are enough close if yes stop all\n",
    "            break\n",
    "        \n",
    "        centroids = new_centroids # update centroide \n",
    "    \n",
    "    return labels, centroids\n",
    "\n",
    "coordinates = new_data[['latitude', 'longitude']].values\n",
    "n_clusters = 5\n",
    "\n",
    "predicted_labels, cluster_centroids = kmeans_clustering(coordinates, n_clusters)\n",
    "\n",
    "new_data['cluster_labels'] = predicted_labels\n",
    "print(new_data['cluster_labels'].value_counts())\n",
    "\n",
    "# Création de la figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Ajout des points de données\n",
    "for cluster_label in range(n_clusters):\n",
    "    cluster_data = new_data[new_data['cluster_labels'] == cluster_label]\n",
    "    fig.add_trace(go.Scattergeo(\n",
    "        lon = cluster_data['longitude'],\n",
    "        lat = cluster_data['latitude'],\n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 8,\n",
    "            color = cluster_label,\n",
    "            line = dict(width = 1, color = 'rgba(0, 0, 0, 0.5)'),\n",
    "            opacity = 0.8\n",
    "        ),\n",
    "        name = f'Cluster {cluster_label}'\n",
    "    ))\n",
    "\n",
    "# Mise en forme de la carte\n",
    "fig.update_layout(\n",
    "    title = 'Carte des clusters via scratch',\n",
    "    geo = dict(\n",
    "        resolution = 110,\n",
    "        showland = True,\n",
    "        showlakes = True,\n",
    "        landcolor = 'rgb(243, 243, 243)',\n",
    "        countrycolor = 'rgb(204, 204, 204)',\n",
    "        lakecolor = 'rgb(255, 255, 255)',\n",
    "        projection_type = 'natural earth'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.write_html('map_clusters.html')\n",
    "webbrowser.open(\"map_clusters.html\")\n",
    "print('map_clusters.html')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
