{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nettoyage variables\n",
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#read csv file\n",
    "data = pd.read_csv('data/stat_acc_V3_cleared.csv', sep=';')\n",
    "new_data = pd.read_csv('data/stat_acc_V3_new.csv', sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#executer le code du fichier preparation.py\n",
    "%run preparation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribut 1: an_nais\n",
      "Attribut 2: age\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de colonnes avant suppression:  21\n",
      "Nombre de colonnes après suppression:  15\n",
      "Pourcentage de réduction:  28.57 %\n"
     ]
    }
   ],
   "source": [
    "# Identifier les attributs fortement corrélés\n",
    "\n",
    "# Créer une matrice de corrélation\n",
    "corr_matrix = new_data.corr().abs()\n",
    "\n",
    "# Identifiez les paires d'attributs fortement corrélés\n",
    "threshold = 0.8  # Définissez votre propre seuil de corrélation\n",
    "highly_correlated_pairs = []\n",
    "\n",
    "# Parcourir la matrice de corrélation\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[i, j]) >= threshold:\n",
    "            pair = (corr_matrix.columns[i], corr_matrix.columns[j])\n",
    "            highly_correlated_pairs.append(pair)\n",
    "\n",
    "#verifier les paires d'attributs fortement corrélés\n",
    "# print(highly_correlated_pairs)\n",
    "\n",
    "# Afficher les paires d'attributs fortement corrélés\n",
    "for pair in highly_correlated_pairs:\n",
    "    print(\"Attribut 1:\", pair[0])\n",
    "    print(\"Attribut 2:\", pair[1])\n",
    "\n",
    "    #suppression d'une des deux colonnes fortement corrélées (ici on supprime la colonne 'an_nais' jugée moins pertinente)\n",
    "    new_data = new_data.drop(pair[0], axis=1)\n",
    "\n",
    "    print()\n",
    "\n",
    "#export csv new_data\n",
    "new_data.to_csv('data/stat_acc_V3_new.csv', sep=';', index=False)\n",
    "\n",
    "#obtenir le nombre de colonnes avant et après suppression\n",
    "print(\"Nombre de colonnes avant suppression: \", len(data.columns))\n",
    "print(\"Nombre de colonnes après suppression: \", len(new_data.columns))\n",
    "\n",
    "#print pourcentage de réduction\n",
    "print(\"Pourcentage de réduction: \", round((len(data.columns) - len(new_data.columns)) / len(data.columns) * 100, 2), \"%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus : PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Num_Acc'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecomposition\u001b[39;00m \u001b[39mimport\u001b[39;00m PCA\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m StandardScaler\n\u001b[1;32m----> 5\u001b[0m X \u001b[39m=\u001b[39m new_data\u001b[39m.\u001b[39;49mdrop(\u001b[39m'\u001b[39;49m\u001b[39mNum_Acc\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m      7\u001b[0m \u001b[39m# # Afficher graphique de visualisation avant réduction\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m# plt.figure(figsize=(10, 8))\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m# plt.scatter(X, data, alpha=0.2)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m# plt.show()\u001b[39;00m\n\u001b[0;32m     12\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()\n",
      "File \u001b[1;32mc:\\Users\\guiro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\guiro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:4954\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4806\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m   4807\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(\n\u001b[0;32m   4808\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4815\u001b[0m     errors: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   4816\u001b[0m ):\n\u001b[0;32m   4817\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4818\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   4819\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4952\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   4953\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4954\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[0;32m   4955\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   4956\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   4957\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   4958\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   4959\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   4960\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[0;32m   4961\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   4962\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\guiro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:4267\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4265\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   4266\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 4267\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4269\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[0;32m   4270\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\guiro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:4311\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, consolidate, only_slice)\u001b[0m\n\u001b[0;32m   4309\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mdrop(labels, level\u001b[39m=\u001b[39mlevel, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m   4310\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 4311\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49mdrop(labels, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4312\u001b[0m     indexer \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4314\u001b[0m \u001b[39m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4315\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\guiro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6644\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6642\u001b[0m \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[0;32m   6643\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 6644\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(labels[mask])\u001b[39m}\u001b[39;00m\u001b[39m not found in axis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6645\u001b[0m     indexer \u001b[39m=\u001b[39m indexer[\u001b[39m~\u001b[39mmask]\n\u001b[0;32m   6646\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Num_Acc'] not found in axis\""
     ]
    }
   ],
   "source": [
    "#Réduction de dimension avec PCA sur le dataset data avec graphique de visualisation avant et après réduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = new_data.drop('Num_Acc', axis=1)\n",
    "\n",
    "# # Afficher graphique de visualisation avant réduction\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# plt.scatter(X, data, alpha=0.2)\n",
    "# plt.show()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "# Créer un objet PCA avec 2 composantes\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Appliquer PCA aux données\n",
    "principalComponents = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Créer un dataframe avec les composantes principales\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "# Afficher les 5 premières lignes du dataframe\n",
    "print(principalDf.head())\n",
    "\n",
    "#afficher graphique de visualisation après réduction\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(principalDf['principal component 1'], principalDf['principal component 2'], alpha=0.2)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering avec sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4    32418\n",
      "0    24409\n",
      "3    16053\n",
      "1      502\n",
      "2      258\n",
      "Name: cluster_labels, dtype: int64\n",
      "map_clusters.html\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "# https://www.kaggle.com/code/lucaspcarlini/clustering-and-visualisation-using-folium-maps\n",
    "import plotly.graph_objects as go\n",
    "import webbrowser\n",
    "from sklearn.cluster import KMeans\n",
    "coordinates = data[['latitude', 'longitude']]\n",
    "# for make cluster variable\n",
    "n = 5 # 5 semble adéquat pour avoir DOM TOM + séparation nord sud centre France\n",
    "kmeans = KMeans(n_clusters=n, random_state=0, n_init=\"auto\").fit(coordinates)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "new_data['cluster_labels'] = labels # get label from dataframe\n",
    "print(new_data['cluster_labels'].value_counts())\n",
    "\n",
    "#preds = kmeans.fit_predict(new_data)\n",
    "\n",
    "\n",
    "# Centroide \n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# graphiques de visualisation des clusters, avec les attributs latitude et longitude + centroide\n",
    "#plt.scatter(new_data['latitude'], new_data['longitude'], c=new_data['cluster_labels'], s=50, cmap='viridis')\n",
    "#plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=100)\n",
    "#plt.show()\n",
    "#predicted_labels, cluster_centroids = kmeans_clustering(coordinates, n_clusters)\n",
    "\n",
    "# Création de la figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Ajout des points de données\n",
    "for cluster_label in range(n):\n",
    "    cluster_data = new_data[new_data['cluster_labels'] == cluster_label]\n",
    "    fig.add_trace(go.Scattergeo(\n",
    "        lon = cluster_data['longitude'],\n",
    "        lat = cluster_data['latitude'],\n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 8,\n",
    "            color = cluster_label,\n",
    "            line = dict(width = 1, color = 'rgba(0, 0, 0, 0.5)'),\n",
    "            opacity = 0.8\n",
    "        ),\n",
    "        name = f'Cluster {cluster_label}'\n",
    "    ))\n",
    "\n",
    "# Ajout des croix rouges pour les centroides\n",
    "for centroid_label, centroid in enumerate(centroids):\n",
    "    fig.add_trace(go.Scattergeo(\n",
    "        lon=[centroid[1]],\n",
    "        lat=[centroid[0]],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=8,\n",
    "            color='red',\n",
    "            symbol='x',\n",
    "        ),\n",
    "    ))\n",
    "\n",
    "# Mise en forme de la carte\n",
    "fig.update_layout(\n",
    "    title = 'Carte des clusters via K-Means sklearn',\n",
    "    geo = dict(\n",
    "        resolution = 110,\n",
    "        showland = True,\n",
    "        showlakes = True,\n",
    "        landcolor = 'rgb(243, 243, 243)',\n",
    "        countrycolor = 'rgb(204, 204, 204)',\n",
    "        lakecolor = 'rgb(255, 255, 255)',\n",
    "        projection_type = 'natural earth'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.write_html('map_clusters.html')\n",
    "webbrowser.open(\"map_clusters.html\")\n",
    "print('map_clusters.html')\n",
    "\n",
    "preds = kmeans.fit_predict(new_data)\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_avg = silhouette_score(new_data, preds)\n",
    "print(\"Silhouette Coefficient :\", silhouette_avg)\n",
    "\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "ch_score = calinski_harabasz_score(new_data, preds)\n",
    "print(\"Calinski-Harabasz Index :\", ch_score)\n",
    "\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "db_score = davies_bouldin_score(new_data, preds)\n",
    "print(\"Davies-Bouldin Index :\", db_score)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering en scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Silhouette Coefficient entre -1 et 1 plus on est proche de 1 plus on est précis \n",
    "- davies_bouldin entre 0 et + inf plus on est proche de 0 mieux c'est \n",
    "- calinski_harabas entre 0 et + inf plus on est proche de inf mieux c'est "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_labels\n",
      "3.0    36692\n",
      "0.0    19503\n",
      "2.0    16685\n",
      "4.0      502\n",
      "1.0      258\n",
      "Name: count, dtype: int64\n",
      "map_clusters.html\n"
     ]
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "import webbrowser\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dist Manhattan\n",
    "def distL1(lat1, lon1, lat2, lon2):\n",
    "    return abs(lat2 - lat1) + abs(lon2 - lon1)\n",
    "\n",
    "# Dist euclidienne\n",
    "def distL2(lat1, lon1, lat2, lon2):\n",
    "    return math.sqrt((lat2 - lat1)**2 + (lon2 - lon1)**2)\n",
    "\n",
    "# Dist Haversine (distance entre 2 pts sur la Terre)\n",
    "def distHaversine(lat1, lon1, lat2, lon2):\n",
    "    radius = 6371  # Rayon de la Terre en kilomètres\n",
    "    dlat = math.radians(lat2 - lat1)    # distance btw 2 pts en radians\n",
    "    dlon = math.radians(lon2 - lon1)\n",
    "    a = math.sin(dlat / 2) * math.sin(dlat / 2) + math.cos(math.radians(lat1)) \\\n",
    "        * math.cos(math.radians(lat2)) * math.sin(dlon / 2) * math.sin(dlon / 2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    d = radius * c\n",
    "    return d\n",
    "\n",
    "def kmeans_clustering(new_data, n_clusters, max_iterations=100):\n",
    "    np.random.seed(0)\n",
    "    centroids = new_data[np.random.choice(range(len(new_data)), size=n_clusters, replace=False)]\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        labels = np.zeros(len(new_data)) # Init labels pour chaque point\n",
    "         # Assignation des points aux clusters les plus proches\n",
    "        for i, point in enumerate(new_data):\n",
    "            min_distance = float('inf')\n",
    "            for j, centroid in enumerate(centroids):\n",
    "                dist = distL1(point[0], point[1], centroid[0], centroid[1]) # Manhattan distance\n",
    "                #dist = distL2(point[0], point[1], centroid[0], centroid[1])# euclienean distance\n",
    "                #dist = distHaversine(point[0], point[1], centroid[0], centroid[1]) # dist btw point and centroid\n",
    "                if dist < min_distance:\n",
    "                    min_distance = dist\n",
    "                    labels[i] = j  # Attribution du label du cluster au point\n",
    "        # Update centroids \n",
    "        new_centroids = np.zeros_like(centroids) # Calcul des nouveaux centroids en prenant la moyenne des points dans chaque cluster\n",
    "        cluster_counts = np.zeros(n_clusters)\n",
    "        for i in range(len(new_data)):\n",
    "            cluster_index = int(labels[i])\n",
    "            new_centroids[cluster_index] += new_data[i] # add coord point to cluster\n",
    "            cluster_counts[cluster_index] += 1 # Count  nb point in each cluster\n",
    "        for j in range(n_clusters):\n",
    "            if cluster_counts[j] > 0: # if cluster not empty to avoid div by 0 \n",
    "                    new_centroids[j] = np.divide(new_centroids[j], cluster_counts[j]) # average of point coord to get new centroid by dividing by nb point in cluster\n",
    "        \n",
    "        if np.allclose(centroids, new_centroids): # compare if old and new centroids are enough close if yes stop all\n",
    "            break\n",
    "        \n",
    "        centroids = new_centroids # update centroide \n",
    "    \n",
    "    return labels, centroids\n",
    "\n",
    "coordinates = new_data[['latitude', 'longitude']].values\n",
    "n_clusters = 5\n",
    "\n",
    "predicted_labels, cluster_centroids = kmeans_clustering(coordinates, n_clusters)\n",
    "\n",
    "new_data['cluster_labels'] = predicted_labels\n",
    "print(new_data['cluster_labels'].value_counts())\n",
    "\n",
    "# Création de la figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Ajout des points de données\n",
    "for cluster_label in range(n_clusters):\n",
    "    cluster_data = new_data[new_data['cluster_labels'] == cluster_label]\n",
    "    fig.add_trace(go.Scattergeo(\n",
    "        lon = cluster_data['longitude'],\n",
    "        lat = cluster_data['latitude'],\n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 8,\n",
    "            color = cluster_label,\n",
    "            line = dict(width = 1, color = 'rgba(0, 0, 0, 0.5)'),\n",
    "            opacity = 0.8\n",
    "        ),\n",
    "        name = f'Cluster {cluster_label}'\n",
    "    ))\n",
    "    \n",
    "# Ajout des croix rouges pour les centroides\n",
    "for centroid_label, centroid in enumerate(centroids):\n",
    "    fig.add_trace(go.Scattergeo(\n",
    "        lon=[centroid[1]],\n",
    "        lat=[centroid[0]],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=8,\n",
    "            color='red',\n",
    "            symbol='x',\n",
    "            line=dict(width=1, color='red'),\n",
    "        ),\n",
    "    ))\n",
    "# Mise en forme de la carte\n",
    "fig.update_layout(\n",
    "    title = 'Carte des clusters via scratch',\n",
    "    geo = dict(\n",
    "        resolution = 110,\n",
    "        showland = True,\n",
    "        showlakes = True,\n",
    "        landcolor = 'rgb(243, 243, 243)',\n",
    "        countrycolor = 'rgb(204, 204, 204)',\n",
    "        lakecolor = 'rgb(255, 255, 255)',\n",
    "        projection_type = 'natural earth'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.write_html('map_clusters.html')\n",
    "webbrowser.open(\"map_clusters.html\")\n",
    "print('map_clusters.html')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
